{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "data_wrangling.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQe9bu_ReHDG"
      },
      "source": [
        "## 1. Data Wrangling\n"
      ],
      "id": "sQe9bu_ReHDG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntb_cpFieXS_"
      },
      "source": [
        "In this part of notebook I will cover how to wrangle data so we can extract the feature easily. We will try to label each event whether it is belong to users subscription phase.\r\n",
        "\r\n",
        "example:\r\n",
        "\r\n",
        "| userId | upgrade_time | downgrade_time | ...event |\r\n",
        "|--------|--------------|----------------|----------|\r\n",
        "|  1111  |  2020-12-05  |   2020-12-29   | ...event |\r\n",
        "|  2222  |  2020-11-12  |      null      | ...event |\r\n",
        "|  3333  |  2020-10-15  |   2020-10-29   | ...event |\r\n",
        "\r\n",
        "the null value in downgrade time means that the user isn't churning"
      ],
      "id": "ntb_cpFieXS_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "281raco_hK_m"
      },
      "source": [
        "### Import Needed Library and Initialiaze PySpark"
      ],
      "id": "281raco_hK_m"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guided-tablet"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "id": "guided-tablet",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "graduate-independence"
      },
      "source": [
        "spark = SparkSession.builder.appName('sparkify') \\\n",
        "    .config('spark.driver.maxResultSize', '3g') \\\n",
        "    .getOrCreate()"
      ],
      "id": "graduate-independence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apart-exercise"
      },
      "source": [
        "### Load the Dataset from Google Cloud Storage"
      ],
      "id": "apart-exercise"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "surrounded-genre",
        "outputId": "433bec42-15a4-4d68-b877-469b7445a80e"
      },
      "source": [
        "# Load dataset from GCS and change ts from bigint to datetime format\n",
        "df = spark.read.parquet('gs://udacity-dsnd/sparkify_event_data.parquet/')\n",
        "df = df.withColumnRenamed(\"ts\",\"ts_temp\").withColumn(\"ts\", (F.col(\"ts_temp\") / 1000).cast(T.TimestampType())).drop(\"ts_temp\")\n",
        "df.cache()"
      ],
      "id": "surrounded-genre",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[artist: string, auth: string, firstName: string, gender: string, itemInSession: bigint, lastName: string, length: double, level: string, location: string, method: string, page: string, registration: bigint, sessionId: bigint, song: string, status: bigint, userAgent: string, userId: string, ts: timestamp]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "authentic-oracle",
        "outputId": "adb325c1-0efc-4d42-99ae-2b9f5d22c31d"
      },
      "source": [
        "df.printSchema()"
      ],
      "id": "authentic-oracle",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- artist: string (nullable = true)\n",
            " |-- auth: string (nullable = true)\n",
            " |-- firstName: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- itemInSession: long (nullable = true)\n",
            " |-- lastName: string (nullable = true)\n",
            " |-- length: double (nullable = true)\n",
            " |-- level: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            " |-- method: string (nullable = true)\n",
            " |-- page: string (nullable = true)\n",
            " |-- registration: long (nullable = true)\n",
            " |-- sessionId: long (nullable = true)\n",
            " |-- song: string (nullable = true)\n",
            " |-- status: long (nullable = true)\n",
            " |-- userAgent: string (nullable = true)\n",
            " |-- userId: string (nullable = true)\n",
            " |-- ts: timestamp (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0I-t8u2hzQ0"
      },
      "source": [
        "### Wrangle Dataframe to Get Event Labeled Dataframe"
      ],
      "id": "u0I-t8u2hzQ0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKJ2601ZiPCO"
      },
      "source": [
        "First, we want to find when each user upgrading and downgrading subscription"
      ],
      "id": "SKJ2601ZiPCO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rotary-palmer"
      },
      "source": [
        "# up_df is data when user upgrading\n",
        "# | userId |        ts         |      page      |\n",
        "# | 234124 | 2018-1-1 12:00:00 | Submit Upgrade |\n",
        "up_df = df.select([\"userId\", \"ts\"]) \\\n",
        "  .filter(df.page == \"Submit Upgrade\") \\\n",
        "  .withColumnRenamed('ts', 'up_ts')\n",
        "\n",
        "# down_df is data when user upgrading\n",
        "# | userId |        ts         |       page       |\n",
        "# | 234124 | 2018-1-5 12:00:00 | Submit Downgrade |\n",
        "down_df = df.select([\"userId\", \"ts\"]) \\\n",
        "    .filter(df.page == \"Submit Downgrade\") \\\n",
        "    .withColumnRenamed('ts', 'down_ts') \\\n",
        "    .withColumnRenamed(\"userId\", \"userIdTemp\")\n"
      ],
      "id": "rotary-palmer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfdleuHeiciw"
      },
      "source": [
        "Second, We query to get every upgrade event and the following downgrade event time "
      ],
      "id": "LfdleuHeiciw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "protected-knowing",
        "outputId": "59ad755a-36c2-4643-b76f-3535cd36cd96"
      },
      "source": [
        "# key_df join up_df and down_df to create dataframe when user upgrade and following downgrade in the same row like below\n",
        "# | userId |      up_ts        |      down_ts      |  isChurn  |\n",
        "# | 234124 | 2018-1-2 12:00:00 | 2018-1-5 12:00:00 |   True    |\n",
        "# | 234124 | 2018-1-6 12:00:00 | 2018-1-9 12:00:00 |   True    |\n",
        "key_df = up_df.join(down_df,\n",
        "    (down_df.userIdTemp == up_df.userId) & \n",
        "    (down_df.down_ts > up_df.up_ts), how=\"left\") \\\n",
        "  .drop(F.col(\"userIdTemp\")) \\\n",
        "  .groupBy(F.col(\"userId\"), up_df.up_ts) \\\n",
        "  .agg(F.min(down_df.down_ts)) \\\n",
        "  .withColumnRenamed(\"max(userId)\", \"userId\") \\\n",
        "  .withColumn(\"down_ts\", \n",
        "    F.when(F.col(\"min(down_ts)\").isNull(), '2099-12-31 00:00:00') \\\n",
        "    .otherwise(F.col(\"min(down_ts)\"))) \\\n",
        "  .withColumn(\"isChurn\", \n",
        "    F.when(F.col(\"min(down_ts)\").isNull(), False).otherwise(True)) \\\n",
        "  .orderBy(up_df.up_ts)\n",
        "\n",
        "key_df.cache()"
      ],
      "id": "protected-knowing",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[userId: string, up_ts: timestamp, min(down_ts): timestamp, down_ts: string, isChurn: boolean]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "banned-riverside",
        "outputId": "258ff8ae-09af-440e-f894-f2f5c59b944a"
      },
      "source": [
        "key_df.show()"
      ],
      "id": "banned-riverside",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------------------+-------------------+-------------------+-------+\n",
            "| userId|              up_ts|       min(down_ts)|            down_ts|isChurn|\n",
            "+-------+-------------------+-------------------+-------------------+-------+\n",
            "|1448964|2018-10-01 00:00:55|2018-11-18 05:04:25|2018-11-18 05:04:25|   true|\n",
            "|1712107|2018-10-01 00:02:02|               null|2099-12-31 00:00:00|  false|\n",
            "|1652185|2018-10-01 00:06:41|               null|2099-12-31 00:00:00|  false|\n",
            "|1851656|2018-10-01 00:11:30|2018-11-01 10:24:19|2018-11-01 10:24:19|   true|\n",
            "|1588246|2018-10-01 00:13:55|2018-10-01 12:27:24|2018-10-01 12:27:24|   true|\n",
            "|1585800|2018-10-01 00:15:59|               null|2099-12-31 00:00:00|  false|\n",
            "|1808085|2018-10-01 00:21:03|2018-10-18 23:55:46|2018-10-18 23:55:46|   true|\n",
            "|1995340|2018-10-01 00:22:00|2018-10-09 07:40:04|2018-10-09 07:40:04|   true|\n",
            "|1417592|2018-10-01 00:30:01|2018-11-30 08:43:17|2018-11-30 08:43:17|   true|\n",
            "|1875033|2018-10-01 00:30:48|               null|2099-12-31 00:00:00|  false|\n",
            "|1620814|2018-10-01 00:33:39|2018-10-02 23:05:28|2018-10-02 23:05:28|   true|\n",
            "|1559837|2018-10-01 00:34:40|               null|2099-12-31 00:00:00|  false|\n",
            "|1924026|2018-10-01 00:39:54|2018-10-05 17:26:52|2018-10-05 17:26:52|   true|\n",
            "|1378685|2018-10-01 00:43:14|               null|2099-12-31 00:00:00|  false|\n",
            "|1788685|2018-10-01 00:43:47|               null|2099-12-31 00:00:00|  false|\n",
            "|1002688|2018-10-01 00:49:38|               null|2099-12-31 00:00:00|  false|\n",
            "|1514504|2018-10-01 00:57:05|               null|2099-12-31 00:00:00|  false|\n",
            "|1388745|2018-10-01 00:57:21|               null|2099-12-31 00:00:00|  false|\n",
            "|1075085|2018-10-01 01:08:20|2018-10-17 12:56:36|2018-10-17 12:56:36|   true|\n",
            "|1699458|2018-10-01 01:19:56|               null|2099-12-31 00:00:00|  false|\n",
            "+-------+-------------------+-------------------+-------------------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "random-disney",
        "outputId": "46c6e6bc-7801-4e4f-c945-0f21ecdb40cf"
      },
      "source": [
        "# save the result then read it again to reduce query complexity\n",
        "key_df.drop(\"min(down_ts)\").write.parquet(\"gs://udacity-dsnd/key_df.parquet\")\n",
        "key_df = spark.read.parquet(\"hdfs:///user/key_df.parquet\")\n",
        "key_df.printSchema()"
      ],
      "id": "random-disney",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- userId: string (nullable = true)\n",
            " |-- up_ts: timestamp (nullable = true)\n",
            " |-- down_ts: string (nullable = true)\n",
            " |-- isChurn: boolean (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObGW3_ylrmV9"
      },
      "source": [
        "Last, we query to label every event with value from key_df and save it to GCS"
      ],
      "id": "ObGW3_ylrmV9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "initial-advance"
      },
      "source": [
        "# label every event that fall between key_df's up_ts and down_ts with the same userId and save it to GCS\n",
        "# the resulting table will look like below\n",
        "# | userId |      up_ts        |      down_ts      |  isChurn  | event  |\n",
        "# | 234124 | 2018-1-5 12:00:00 | 2018-1-5 12:00:00 |   True    | event1 |\n",
        "# | 234124 | 2018-1-5 12:00:00 | 2018-1-5 12:00:00 |   True    | event2 |\n",
        "df = df.withColumnRenamed(\"userId\", \"userIdTemp\")\n",
        "key_df.join(df, (key_df.up_ts <= df.ts) & (df.ts <= key_df.down_ts) & (key_df.userId == df.userIdTemp),how='left') \\\n",
        "  .write.parquet('gs://udacity-dsnd/event_labeled.parquet')"
      ],
      "id": "initial-advance",
      "execution_count": null,
      "outputs": []
    }
  ]
}